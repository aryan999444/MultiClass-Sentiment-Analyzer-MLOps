{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed9a9d35",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec3780d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4ea63a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n",
      "Initail shape of the dataset: (205052, 6)\n",
      "\n",
      "Initial Columns:\n",
      "Index(['product_name', 'product_price', 'Rate', 'Review', 'Summary',\n",
      "       'Sentiment'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "file_path = '../data/Dataset-SA.csv'\n",
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "    print(f\"Initail shape of the dataset: {df.shape}\")\n",
    "    print(\"\\nInitial Columns:\")\n",
    "    print(df.columns)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file at {file_path} was not found.\")\n",
    "\n",
    "# print(f\"Initail shape of the dataset: {df.shape}\")\n",
    "# print(\"\\nInitial Columns:\")\n",
    "# print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68de6834",
   "metadata": {},
   "source": [
    "#### Rename Columns and Handle Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "604e0854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values before cleaning:\n",
      "product_name           0\n",
      "product_price          0\n",
      "Rate                   0\n",
      "review_text        24664\n",
      "Summary               11\n",
      "Sentiment_label        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = df.rename(columns={'Review': 'review_text', 'Sentiment': 'Sentiment_label'})\n",
    "print(\"\\nMissing values before cleaning:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "291c9ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['review_text', 'Summary'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68c6e1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape after dropping NaNs: (180379, 6)\n",
      "\n",
      "Final missing values check:\n",
      "product_name       0\n",
      "product_price      0\n",
      "Rate               0\n",
      "review_text        0\n",
      "Summary            0\n",
      "Sentiment_label    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nShape after dropping NaNs: {df.shape}\")\n",
    "print(\"\\nFinal missing values check:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b86e85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       review_text                                            Summary\n",
      "0           super!  great cooler excellent air flow and for this p...\n",
      "1          awesome              best budget 2 fit cooler nice cooling\n",
      "2             fair  the quality is good but the power of air is de...\n",
      "3  useless product                  very bad product its a only a fan\n",
      "4             fair                                      ok ok product\n"
     ]
    }
   ],
   "source": [
    "print(df[['review_text', 'Summary']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c8e3add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentiment label distribution(target variable):\n",
      "Sentiment_label\n",
      "positive    147171\n",
      "negative     24401\n",
      "neutral       8807\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSentiment label distribution(target variable):\")\n",
    "label_counts = df['Sentiment_label'].value_counts()\n",
    "print(label_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45cfa03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribution Percentage:\n",
      "Sentiment_label\n",
      "positive    81.589875\n",
      "negative    13.527628\n",
      "neutral      4.882497\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "total_count = label_counts.sum()\n",
    "print(\"\\nDistribution Percentage:\")\n",
    "print((label_counts / total_count) * 100)\n",
    "# for label, count in label_counts.items():\n",
    "#     percentage = (count / total_count) * 100\n",
    "#     print(f\"Label: {label}, Count: {count}, Percentage: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d608c516",
   "metadata": {},
   "source": [
    "#### Phase 2 - Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375a3947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f3b67cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aryan\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK 'stopwords' downloaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords') \n",
    "print(\"NLTK 'stopwords' downloaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19f26399",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except nltk.downloader.DownloadError:\n",
    "    nltk.download('stopwords') \n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4fa2ce42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^a-z\\s]', ' ',text)\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n",
    "    text = text.strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2035426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw Text vs. Cleaned Text:\n",
      "       review_text     cleaned_text\n",
      "0           super!            super\n",
      "1          awesome          awesome\n",
      "2             fair             fair\n",
      "3  useless product  useless product\n",
      "4             fair             fair\n",
      "----------------------\n",
      "            review_text cleaned_text\n",
      "205047        must buy!     must buy\n",
      "205048           super!        super\n",
      "205049             nice         nice\n",
      "205050        just wow!          wow\n",
      "205051  value-for-money  value money\n"
     ]
    }
   ],
   "source": [
    "# Apply cleaning function\n",
    "df['cleaned_text'] = df['review_text'].apply(clean_text)\n",
    "# show effect of cleaning\n",
    "print(\"\\nRaw Text vs. Cleaned Text:\")\n",
    "print(df[['review_text', 'cleaned_text']].head())\n",
    "print(\"----------------------\")\n",
    "print(df[['review_text', 'cleaned_text']].tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f3153d",
   "metadata": {},
   "source": [
    "#### Phase 3: Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f0e2ee",
   "metadata": {},
   "source": [
    "##### Define X (features) and y (target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc5ac409",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['cleaned_text']\n",
    "y = df['Sentiment_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "232d2250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=44, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0be855e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (144303,)\n",
      "X_test shape: (36076,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f41b4dc",
   "metadata": {},
   "source": [
    "##### TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95dfa9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2))\n",
    "X_train_vectrorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b023dcb",
   "metadata": {},
   "source": [
    "#### Exporting and save variable by joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c4b6bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from scipy.sparse import save_npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b63cfb0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../model/vectorizer.joblib']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Saving the Vectorized Data ---\n",
    "joblib.dump(y_train, '../model/y_train.joblib')\n",
    "joblib.dump(y_test, '../model/y_test.joblib')\n",
    "joblib.dump(vectorizer, '../model/vectorizer.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "effc6fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_npz('../model/X_train_vectorized.npz', X_train_vectrorized)\n",
    "save_npz('../model/X_test_vectorized.npz', X_test_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cea95a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
